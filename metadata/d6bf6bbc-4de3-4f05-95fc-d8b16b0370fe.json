{"language model": [0.0], "large language": [0.0], "chapter lots": [0.0], "talk chapter": [0.0], "train llm": [0.0], "llm architecture": [30.0], "building llm": [30.0], "tuning llm": [30.0], "training llm": [2490.0, 330.0, 3420.0, 30.0], "coding llm": [30.0], "encoding vectors": [60.0], "text tokenize": [60.0], "convert tokenized": [60.0], "tokenize convert": [60.0], "text token": [1380.0, 60.0, 2580.0, 1050.0, 2460.0], "llm understands": [90.0], "data llm": [1920.0, 90.0, 4020.0], "videos detailed": [90.0], "raw text": [480.0, 420.0, 3780.0, 780.0, 750.0, 90.0], "llm process": [90.0], "representations text": [120.0], "vectors like": [120.0], "idea vectors": [120.0], "mathematical vectors": [120.0], "vectors": [120.0], "tokenization": [1410.0, 150.0, 510.0], "tokenizing": [660.0, 150.0], "tokenizing text": [150.0], "tokenization just": [150.0], "known tokenization": [150.0], "tick tokenizer": [2400.0, 180.0, 2340.0], "tick token": [2790.0, 2280.0, 3720.0, 2220.0, 4140.0, 180.0], "visualization tick": [180.0], "llms example": [180.0], "gpt llm": [180.0], "comma token": [210.0], "token comma": [210.0], "individual tokens": [210.0], "token dot": [210.0], "token ids": [4770.0, 1350.0, 2790.0, 840.0, 4680.0, 4650.0, 2670.0, 210.0, 1170.0, 4470.0, 3000.0, 1050.0], "loaders pytorch": [240.0], "pytorch": [3330.0, 3240.0, 3180.0, 240.0, 3090.0, 3030.0], "individual chunks": [240.0], "data loaders": [240.0], "text individual": [240.0], "python library": [270.0], "internet data": [270.0], "standard python": [270.0], "data set": [450.0, 390.0, 3750.0, 360.0, 270.0, 3630.0, 3570.0, 3510.0], "python": [270.0], "domain data": [330.0, 300.0], "wikipedia": [300.0], "short story": [300.0], "data": [450.0, 390.0, 3750.0, 360.0, 300.0, 3510.0], "tab": [300.0], "llm educational": [330.0], "llm training": [330.0], "example llm": [330.0], "download data": [360.0], "repository book": [360.0], "text file": [360.0], "dash dash": [390.0], "dash": [450.0, 390.0], "llms dash": [390.0], "github raw": [420.0], "github": [420.0], "yeah github": [420.0], "copy url": [420.0], "file dash": [450.0], "dash verdict": [450.0], "python interpreter": [480.0], "encoding using": [480.0], "read raw": [480.0], "say encoding": [480.0], "tokenization regular": [510.0], "479 characters": [510.0], "missing characters": [510.0], "characters loaded": [510.0], "regular expressions": [570.0, 540.0], "regular expression": [600.0, 1170.0, 540.0, 720.0], "python regular": [540.0], "simpler text": [540.0], "result regular": [540.0], "training llms": [570.0], "llms": [570.0, 2670.0], "llms like": [570.0], "nowadays llms": [570.0], "whitespace characters": [600.0, 660.0], "words whitespace": [600.0], "split text": [600.0], "whitespace": [600.0], "punctuation separate": [630.0], "include punctuation": [630.0], "separate characters": [630.0], "punctuation": [630.0], "tokens word": [630.0], "strip whitespace": [660.0], "rid whitespace": [660.0], "list comprehension": [660.0], "characters result": [690.0], "encoding": [1920.0, 1800.0, 690.0, 2580.0, 2100.0], "characters bit": [690.0], "space characters": [690.0], "white space": [690.0], "double dash": [720.0], "special characters": [720.0, 1680.0], "sophisticated regular": [720.0], "characters like": [720.0], "tokens punctuation": [750.0], "text raw": [750.0], "punctuation characters": [750.0], "text splitting": [750.0], "tokens length": [780.0], "tokenized raw": [780.0], "tokenized": [4050.0, 810.0, 780.0], "let tokens": [780.0], "tokenized section": [810.0], "text tokenized": [4050.0, 810.0], "tokens": [810.0, 1680.0, 2160.0, 1170.0, 1620.0, 2010.0, 2430.0], "690 tokens": [810.0], "token": [4740.0, 840.0, 4140.0, 2160.0, 1200.0, 1170.0], "ids": [840.0], "sort alphabetically": [870.0], "alphabetically": [870.0], "alphabetically making": [870.0], "vocabulary unique": [870.0], "sort": [870.0], "duplicates set": [900.0], "assign unique": [900.0], "rid duplicates": [900.0], "unique tokens": [900.0, 1620.0], "process unique": [900.0], "unique words": [960.0, 930.0], "vocabulary": [930.0], "words variable": [930.0], "assign words": [930.0], "bunch duplicates": [930.0], "token enumerate": [960.0], "vocabulary size": [960.0, 4110.0], "integer token": [960.0], "build vocabulary": [960.0], "words assign": [990.0], "words iterate": [990.0], "word integer": [990.0], "mapping word": [990.0], "integer labels": [990.0], "vocabulary mapping": [1020.0], "vocabulary sorted": [1020.0], "use llm": [1020.0], "training text": [4500.0, 1020.0], "tokenize training": [1020.0], "encode given": [1050.0], "word brown": [1050.0], "brown correspond": [1050.0], "tokenizer class": [1080.0], "init usually": [1080.0], "python class": [1080.0], "class python": [1080.0], "tokenizer": [1440.0, 1410.0, 2370.0, 2340.0, 2520.0, 1320.0, 1770.0, 1740.0, 1710.0, 2190.0, 4560.0, 1950.0, 1590.0, 2550.0, 1080.0, 1500.0, 1470.0], "object constructor": [1110.0], "string integer": [1230.0, 1110.0], "strings integer": [1110.0], "constructor": [1110.0], "constructor takes": [1110.0], "vocabulary encode": [1140.0], "mapping integer": [1140.0], "vocabularies regular": [1140.0], "vocabulary inverted": [1140.0], "inverted vocabulary": [1140.0], "text tokens": [1170.0, 2610.0, 1590.0], "token id": [1200.0, 4740.0, 4830.0], "essentially token": [1200.0], "token string": [1200.0], "vocabulary token": [1200.0], "integer string": [1290.0, 1230.0], "type 57": [1230.0], "string type": [1230.0], "vocabulary inverse": [1230.0], "string token": [1260.0], "decode method": [1260.0], "decode": [1260.0], "method decode": [1260.0], "really decode": [1260.0], "string": [1290.0], "string think": [1290.0], "object using": [1290.0], "object": [1290.0], "tokenizer encode": [1320.0], "execute cell": [1320.0], "calling encode": [1320.0], "let tokenizer": [1320.0], "tokenizer decode": [2400.0, 3000.0, 1770.0, 1350.0], "decode ids": [1350.0], "ids tokenizer": [1350.0], "converting text": [1380.0, 1350.0], "decode encoding": [1380.0], "encoding inside": [1380.0], "original text": [1800.0, 1380.0], "gpt models": [1410.0], "simple tokenizer": [1410.0, 2370.0, 1740.0, 1710.0, 1590.0, 1470.0], "real tokenizer": [1410.0], "context tokens": [1440.0], "tokens training": [1440.0], "special tokens": [1440.0, 1710.0], "tokenizer token": [1440.0], "tokenizer defined": [1470.0], "shortcoming tokenizer": [1470.0], "tokenizer based": [1470.0], "sophisticated used": [1500.0], "say sophisticated": [1500.0], "tokenizer does": [1500.0], "sophisticated": [1500.0], "encode error": [1530.0], "encode": [2040.0, 1530.0, 2430.0], "word error": [1530.0], "key error": [1530.0], "error hello": [1530.0], "word training": [1560.0], "words example": [1560.0], "unknown words": [1560.0, 1980.0], "example word": [1560.0], "word unknown": [1560.0], "tokenizer slightly": [1590.0], "tokens list": [1590.0], "additional tokens": [1620.0], "tokens data": [1650.0, 1620.0], "tokens example": [1620.0, 1950.0], "generate vocabulary": [1650.0], "mapping vocabulary": [1650.0], "length vocabulary": [1650.0], "vocabulary similar": [1650.0], "132 tokens": [1680.0], "new tokens": [1680.0], "tokens added": [1680.0], "tokens vocabulary": [1710.0], "tokenizer actually": [1710.0], "string vocabulary": [1740.0], "tokenizer crashes": [1740.0], "unknown token": [1740.0], "tokenizer code": [1770.0], "tokenizer tokenizer": [1770.0], "initializing tokenizer": [1770.0], "unknown tokens": [1800.0, 1890.0], "encoding explain": [1800.0], "pair encoding": [1920.0, 1830.0, 1800.0, 2100.0, 2070.0, 2040.0], "implementing tokenizers": [1830.0], "byte pair": [2100.0, 2580.0, 1830.0], "tokenizers": [1830.0], "algorithm tokenizer": [1830.0], "handled replaced": [1860.0], "way handled": [1860.0], "handled": [1860.0], "replaced": [1860.0], "way": [1860.0], "placeholder tokens": [1890.0], "unk placeholder": [1890.0], "placeholder token": [1890.0], "tokens happens": [1890.0], "encoding algorithm": [1920.0, 2040.0, 2070.0], "tokens text": [1920.0], "sub tokens": [2010.0, 2430.0, 1950.0], "tock tokenizer": [1950.0], "tokenizer app": [1950.0], "gpt": [2850.0, 5250.0, 4200.0, 1980.0, 4590.0, 2460.0, 3390.0], "gpt nonetheless": [1980.0], "data gpt": [1980.0], "tokens failing": [1980.0], "tokens string": [2010.0], "character token": [2010.0], "string like": [2010.0], "encode input": [2040.0], "way encode": [2040.0], "trained tokenizer": [2070.0], "openai gpt": [2070.0], "encoding implemented": [2070.0], "encoding tokenizer": [2400.0, 2100.0], "encoding step": [2100.0], "openai weights": [2130.0], "loading openai": [2130.0], "similarly openai": [2130.0], "openai": [2130.0], "gpt model": [2250.0, 2130.0, 3390.0, 5250.0], "subtokens bpe": [2160.0], "tokens way": [2160.0], "individual subtokens": [2160.0], "tokenizer really": [2190.0], "consider tokenizer": [2190.0], "tokenizer going": [2340.0, 2190.0], "case tokenizer": [2190.0], "import tick": [2280.0, 2220.0], "implemented rust": [2220.0], "token library": [2280.0, 2220.0], "python api": [2220.0], "gpt tokenizer": [2280.0, 2250.0, 2460.0, 2370.0], "architecture gpt": [2250.0], "gpt gpt": [2250.0], "gpt style": [2250.0], "token python": [2280.0], "uv pip": [3090.0, 2310.0], "install tick": [2310.0], "install library": [2310.0], "use uv": [2310.0], "pip install": [2310.0], "tokenizer object": [2340.0], "new tokenizer": [2340.0], "tokenizer vocabulary": [2370.0, 2550.0], "tokenizer like": [2370.0], "api encoding": [2400.0], "encode decode": [2400.0], "encode text": [2430.0], "tokens let": [2430.0], "thing gpt": [2460.0], "tokenizer use": [2460.0], "llm document": [2490.0], "previous document": [2490.0], "document previous": [2490.0], "ends document": [2490.0], "tokenizer complaining": [2520.0], "enabled tokenizer": [2520.0], "token explicit": [2520.0], "special token": [2520.0], "size tokenizer": [4440.0, 2550.0], "arbitrary text": [2550.0], "text corresponds": [2550.0], "nutshell byte": [2580.0], "text subware": [2580.0], "implementing llm": [2610.0], "message algorithm": [2610.0], "llm core": [2610.0], "llm": [2610.0, 4470.0], "sampling": [2640.0], "llm efficiently": [4020.0, 2670.0], "llms predicting": [2670.0], "examples llms": [2670.0], "llms learn": [2700.0], "learn predict": [2700.0], "llm predict": [2700.0], "predict token": [2970.0, 2700.0], "learn token": [2700.0], "create labeled": [2730.0], "token label": [2730.0], "label use": [2730.0], "label create": [2730.0], "text label": [2730.0], "predicting tokens": [2760.0], "label nlm": [2760.0], "nlm learns": [2760.0], "nlm nlm": [2760.0], "input nlm": [2760.0], "verdict data": [2790.0], "ids text": [2790.0], "visualize token": [2790.0], "provide chunks": [2820.0], "tokens llm": [2820.0], "efficiently llm": [2820.0], "sub chunks": [2820.0], "chunks lot": [2820.0], "24 tokens": [2850.0], "50 tokens": [2850.0], "tokens used": [2850.0], "gpt think": [2850.0], "targets inputs": [2880.0, 3870.0], "inputs targets": [2880.0, 3480.0, 3990.0], "set targets": [2880.0], "target token": [2880.0], "targets": [2880.0, 3390.0], "targets token": [2910.0], "token llm": [2970.0, 2910.0], "tokens context": [2910.0], "truncated tokens": [2910.0], "context size": [2910.0], "overlaps tokens": [2940.0], "tokens overlap": [2940.0], "overlap token": [2940.0], "tokens shifted": [2940.0], "position targets": [2940.0], "tokens predict": [2970.0], "token presented": [2970.0], "previous token": [2970.0], "decode token": [3000.0], "showing token": [3000.0], "tokenizer example": [3000.0], "predicting word": [3360.0, 3030.0], "sentence": [3030.0], "sentence make": [3030.0], "word sentence": [3030.0], "torch library": [3060.0], "pytorch imported": [3060.0], "use pytorch": [3060.0, 3420.0], "imported torch": [3060.0], "called pytorch": [3060.0], "pytorch installed": [3090.0], "pytorch org": [3090.0], "example pytorch": [3090.0], "like installer": [3120.0], "installer menu": [3120.0], "installer": [3120.0], "install torch": [3120.0], "cpu code": [3120.0], "gpu cuda": [3150.0], "cuda versions": [3150.0], "support gpu": [3150.0], "cuda 12": [3150.0], "set cuda": [3150.0], "versions pytorch": [3210.0, 3180.0], "pytorch version": [3180.0], "new pytorch": [3240.0, 3180.0], "pytorch couldn": [3180.0], "difference pytorch": [3210.0], "version pytorch": [3240.0, 3210.0], "pytorch doing": [3210.0], "pytorch example": [3210.0], "pytorch using": [3240.0], "pytorch essentially": [3240.0], "pytorch workshops": [3270.0], "taught pytorch": [3270.0], "book pytorch": [3300.0, 3270.0], "books pytorch": [3270.0], "using pytorch": [3420.0, 3270.0], "speed pytorch": [3300.0], "basic llm": [3300.0], "course book": [3300.0], "need basic": [3300.0], "learn pytorch": [3330.0], "pytorch maybe": [3330.0], "particular book": [3330.0], "steeper learning": [3330.0], "word predicting": [3360.0], "using chunk": [3360.0], "chunk": [3360.0], "creating chunks": [3360.0], "targets shifted": [3840.0, 3390.0], "case gpt": [3390.0], "pytorch data": [3420.0], "feed llm": [3420.0], "create batches": [3450.0], "batches shuffle": [3450.0], "batches": [3450.0, 3690.0], "shuffle data": [3450.0], "chunks chunks": [3450.0], "chunk inputs": [3480.0], "targets appending": [3480.0], "chunks adding": [3480.0], "creating chunk": [3480.0], "large data": [3510.0], "input row": [3510.0], "row": [3510.0], "tokens memory": [3540.0], "trillions tokens": [3540.0], "million tokens": [3540.0], "books gigabytes": [3540.0], "memory": [3540.0], "batch size": [3780.0, 4710.0, 3630.0, 3570.0, 3960.0], "data lot": [3570.0], "set batch": [3570.0, 3630.0], "batch": [3840.0, 3570.0, 5100.0], "context length": [3600.0, 3810.0], "length context": [3600.0, 3810.0], "set length": [3600.0], "set stride": [3600.0], "drop batch": [3600.0], "batch sizes": [3630.0], "divisible batch": [3630.0], "multiple epoch": [3660.0], "epoch training": [3660.0], "end batch": [3660.0], "remove batch": [3660.0], "batches end": [3660.0], "batches size": [3690.0], "batch num": [3690.0], "num workers": [3690.0], "dropping batch": [3690.0], "processes python": [3720.0], "background processes": [3720.0], "tokenizer tick": [3720.0], "token tokenizer": [3720.0, 4140.0], "training data": [3750.0], "data loader": [4020.0, 3750.0], "creating data": [3750.0], "samples batch": [3780.0], "stride batch": [3780.0], "batch essentially": [3780.0], "max length": [3810.0, 5010.0], "tokens stride": [3810.0], "stride": [3810.0, 3930.0], "iteration object": [3840.0], "batch input": [3840.0], "executing batch": [3840.0], "batch targets": [3870.0], "targets set": [3870.0], "second targets": [3870.0], "draw batch": [3870.0], "stride tokens": [3900.0], "overlap tokens": [3900.0], "second iteration": [3900.0], "iteration input": [3900.0], "overfitting": [3900.0], "stride positions": [3930.0], "city stood": [3930.0], "stride actually": [3930.0], "just stride": [3930.0], "size batch": [3960.0], "length stride": [3960.0], "stride labeling": [3960.0], "larger batch": [3960.0], "row training": [3990.0], "corresponding targets": [3990.0], "training example": [3990.0], "example batch": [3990.0], "load data": [4020.0], "loader way": [4020.0], "token embeddings": [4770.0, 5220.0, 4710.0, 4560.0, 4050.0, 4950.0, 4920.0], "embeddings token": [4050.0, 5220.0], "tokenized text": [4050.0, 5220.0], "torch tensor": [4080.0, 4350.0], "pytorch tensor": [4080.0], "input tensor": [4080.0], "tensor": [4080.0, 5040.0, 4350.0], "tensor insert": [4080.0], "embedding size": [4200.0, 5130.0, 4620.0, 4110.0], "layer llm": [4110.0], "usually embedding": [4110.0], "size vocabulary": [4110.0], "python deer": [4140.0], "tokenizer 000": [4140.0], "size vectors": [4170.0], "vocab length": [4170.0], "embedding layer": [4320.0, 4290.0, 4260.0, 4170.0, 5070.0, 4920.0, 4980.0, 4440.0, 4890.0], "output dimension": [4200.0, 4170.0, 4590.0], "type vocab": [4170.0], "gpt embedding": [4200.0], "original gpt": [4200.0], "layer random": [4230.0], "random seed": [4230.0], "weights random": [4230.0], "initializing random": [4230.0], "random weights": [4230.0], "layer embedding": [4260.0], "layer weight": [4260.0], "weight parameters": [4260.0], "random seeds": [4260.0], "embedding": [4440.0, 4290.0, 5070.0, 4800.0], "multiplications embedding": [4290.0], "linear layers": [4320.0, 4290.0], "layers matrix": [4290.0], "embedding layers": [4320.0, 5250.0], "linear layer": [4320.0], "layers linear": [4320.0], "tensor different": [4350.0], "embedding matrix": [4620.0, 4350.0], "tensor use": [4350.0], "vector matrix": [4380.0], "matrix": [4410.0, 4380.0], "giving vector": [4380.0], "python zero": [4380.0], "vector": [4380.0], "corresponding row": [4410.0], "index corresponding": [4410.0], "values matrix": [4410.0], "input ids": [4410.0], "calling embedding": [4440.0], "example tokenizer": [4440.0], "ids vectors": [4470.0], "257 rows": [4470.0], "usually llm": [4470.0], "random trained": [4500.0], "llm optimized": [4500.0], "training numbers": [4500.0], "weight matrix": [4800.0, 4500.0, 4620.0, 4830.0], "understanding embedding": [4530.0], "embedding embedding": [4530.0], "word embeddings": [4530.0], "vector embedding": [4530.0], "embeddings": [4650.0, 4530.0, 4980.0, 4950.0, 4890.0, 4860.0], "sizes tokenizer": [4560.0], "embeddings small": [4560.0], "size embedding": [4560.0], "dimension tokens": [4590.0], "dimension think": [4590.0], "dimension": [4590.0], "dimensions embedding": [4620.0], "columns weight": [4620.0], "embeddings previously": [4650.0], "come embeddings": [4650.0], "data lotter": [4650.0], "ids embedding": [4680.0], "token embedding": [4680.0, 4920.0, 5130.0, 5220.0], "batch data": [4680.0], "data batch": [4680.0], "tensor numbers": [4710.0], "correspond batch": [4710.0], "example tokens": [4710.0], "vector token": [4740.0], "batch token": [4740.0], "just token": [4740.0], "token batch": [4770.0], "corresponding token": [4800.0, 4770.0], "converted token": [4770.0], "embedding vectors": [4800.0], "vectors weight": [4800.0], "id vector": [4830.0], "token index": [4830.0], "row token": [4830.0], "positional embedding": [4860.0, 5070.0], "embedding information": [4860.0], "positional embeddings": [4860.0], "second embedding": [4860.0], "rope embeddings": [4890.0], "implemented llama": [4890.0], "embeddings implemented": [4890.0], "position embeddings": [4920.0, 5130.0], "position embedding": [4920.0, 5160.0, 4980.0], "input embeddings": [5220.0, 4950.0], "embeddings look": [4950.0], "example token": [4950.0], "embedding numbers": [4980.0], "length inputs": [4980.0], "torch range": [5010.0], "range create": [5010.0], "range": [5010.0], "range range": [5010.0], "tensor positions": [5040.0], "placeholder tensor": [5040.0], "tensor placeholder": [5040.0], "just tensor": [5040.0], "layer": [5070.0], "positional": [5070.0], "batch dimension": [5100.0], "256 shape": [5100.0], "shape 256": [5100.0], "duplicate batch": [5100.0], "embedding dimension": [5130.0], "embeddings input": [5130.0], "embedding vector": [5160.0], "examples batch": [5160.0], "batch example": [5160.0], "example training": [5160.0], "patch dimension": [5190.0], "print representation": [5190.0], "representation row": [5190.0], "generalizing patch": [5190.0], "row essentially": [5190.0], "inside gpt": [5250.0], "decoder transformer": [5250.0], "video": [5280.0]}